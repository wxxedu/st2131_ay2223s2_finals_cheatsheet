\subsection{Joint Moment Generating Functions}

For any \(n\) random variables \(X_1, X_2, \ldots, X_n\), the \textbf{joint
moment generating function}, \(M_{X_1, \cdots, X_n}(t_1, \cdots, t_n)\), is
defined for all real values of \(t_1, \cdots, t_n\) by
\[M_{X_1, \cdots, X_n}(t_1, \cdots, t_n) = E\left[\exp({t_1X_1 + \cdots +
t_nX_n})\right].\]
The \textbf{individual} moment generating function can be obtained by
setting the other \(t_j\)'s to zero:
\[M_{X_i}(t_i) = M_{X_1, \cdots, X_n}(0, \cdots, t_i, \cdots, 0).\]

\marker{Proposition 7.8 Independence of Mean and Variance from Normal Sample} 
Let \(X_1, X_2, \ldots, X_n\) be independent and identically distributed normal
random variables with mean \(\mu\) and variance \(\sigma^2\). Then the sample
mean \(\overline{X}\) and the sample variance \(S^2\) are independent, and
\(\overline{X}\sim N\left(\mu, \sigma^2/n\right)\) and \((n-1)S^2/\sigma^2
\sim \chi^2(n-1)\).

\section{Limit Theorems}

\subsection{Introduction}

\subsection{Chebyshev's Inequality and the Weak Law of Large Numbers}

\marker{Proposition 8.1 Markov's Inequality} Let \(X\) be a
\textbf{nonnegative} random variable. For \(a > 0\), we have 
\[P(X \geq a)\leq \dfrac{E[X]}{a}.\]

\marker{Proposition 8.2 Chebyshev's Inequality} Let \(X\) be a random variable
with mean \(\mu\) and variance \(\sigma^2\). For \(a > 0\), we have
\[P(|X-\mu| \geq a) \leq \dfrac{\sigma^2}{a^2}.\]

\marker{Proposition 8.3} If \(\operatorname{Var}\left(X\right) = 0\), then the
random variable \(X\) is a constant.

\marker{Theorem 8.1 The Weak Law of Large Numbers} Let \(X_1, X_2, \cdots\) be
a sequence of independent and identically distributed random variables with
common mean \(\mu\). Then, for any \(\epsilon > 0\), we have
\[P\left(\left|\dfrac{X_1+X_2+\cdots+X_n}{n}-\mu\right|\geq
\epsilon\right)\rightarrow 0\]
as \(n\rightarrow\infty\).

\subsection{The Central Limit Theorem}

Let \(X_1, X_2, \cdots\) be a sequence of independent and identically
distributed random variables, each having mean \(\mu\) and variance
\(\sigma^2\). Then the distribution of 
\[\dfrac{X_1+X_2+\cdots+X_n-n\mu}{\sigma\sqrt{n}}\]
tends to the standard norm as \(n \rightarrow \infty\). That is 

\begin{align*}
&\lim\limits_{n\rightarrow
\infty}P\left(\dfrac{X_1+X_2+\cdots+X_n-n\mu}{\sigma\sqrt{n}}\right) \\ 
  =&\dfrac{1}{\sqrt{2\pi}}\int_{x}^{-\infty}\exp(-t^2/2)\ \mathrm{d}t
\end{align*}

\subsection{The Strong Law of Large Numbers}

\marker{Theorem 8.2 The Strong Law of Large Numbers} Let \(X_1, X_2, \cdots\)
be a sequence of independent and identically distributed random variables with
common mean \(\mu\). Then, with probability 1, we have
\[\lim\limits_{n\rightarrow\infty}\dfrac{X_1+X_2+\cdots+X_n}{n}=\mu.\]


\subsection{Other Inequalities}

\marker{Proposition 8.4 One-sided Checbychev's Inequality} Let \(X\) be a
random variable with mean \(0\) and finite variance \(\sigma^2\). Then, for
\(a > 0\), we have
\[P(X \geq a) \leq \dfrac{\sigma^2}{\sigma^2+a^2}.\]

\marker{Proposition 8.5 Jensen's Inequality} If \(g(x)\) is a \textbf{convex}
function, then 
\[g(E[X]) \leq E[g(X)]\]
provided that expectations exist and are finite.

A function is convex if either of the following equivalent conditions hold:
\begin{enumerate}
  \item for all \(0 \leq p \leq 1\) and for all \(x_1, x_2 \in R_X\), 
    \[g(px_1 + (1-p)x_2)\leq pg(x_1) + (1-p)g(x_2).\]
  \item differentiable function: convex of interval if and only if 
    \[g(x) \geq g(y) + g'(y)(x-y)\]
    for all \(x, y\) in the interval.
  \item A twice differentiable function is convex if and only if its second
    derivative is nonnegative.
\end{enumerate}
