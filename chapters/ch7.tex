\marker{Expectation of Func of Joint}:
\begin{itemize}
  \item \textbf{D}: \(E[g(X, Y)] = \sum_y\sum_xg(x,y)p_{X,Y}(x,y)\);
  \item \textbf{C}: \(E[g(X, Y)] =
    \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}g(x,y)f_{X,Y}(x,y)
  \,\mathrm{d}x\mathrm{d}y.\)
\end{itemize}

\marker{Properties}: 
\begin{itemize}
  \item If \(a \leq X \leq b\), then \(a \leq E(X) \leq b\).
  \item If \(g(x,y)\geq 0\) whenever \(p_{X,Y}(x,y) > 0\), then \(E[g(X, Y)]
    \geq 0\).
  \item \(E[g(X, Y) + h(X,Y)] = E[g(X, Y)] + E[h(X,Y)]\).
  \item \(E[g(X) + h(Y)] = E[g(X)] + E[h(Y)]\).
  \item \textbf{Monotone property}: if jointly distributed random variables
    \(X\) and \(Y\) satisfy \(X \leq Y\), then \(E[X] \leq E[Y]\).
  \item \textbf{Special case}: mean of sum is sum of means: \(E[X + Y] = E[X]
    + E[Y]\).
\end{itemize}


\marker{Covariance}: let \(\mu_X = E(X)\) and \(\mu_Y = E(Y)\), then
{\small
\[\operatorname{Cov}\left(X,Y\right):= E[(X-\mu_X)(Y-\mu_Y)] = E(XY) -
E(X)E(Y).\]}
\marker{Properties of Covariance}:
\begin{itemize}
  \item \(\operatorname{Cov}\left(X,Y\right)\neq 0\) \tf \(X\) and \(Y\)
    are \textbf{correlated};
  \item \(\operatorname{Cov}\left(X,Y\right)= 0\) \tf \(X\) and \(Y\) are
    \textbf{uncorrelated}.
  \item \(\operatorname{Var}\left(X\right)=\operatorname{Cov}\left(X,X\right)\)
  \item \(\operatorname{Cov}\left(X,Y\right)=\operatorname{Cov}\left(Y,
    X\right)\)
  \item \(\operatorname{Cov}\left(\sum\limits_{i=1}^{n}a_iX_i,
    \sum\limits_{j=1}^m b_jY_j\right) =
    \sum\limits_{i=1}^{n}\sum\limits_{j=1}^m a_ib_j\operatorname{Cov}\left(
    X_i,Y_j\right).\)
\end{itemize}

\marker{Product of Independent}: If \(X\) and \(Y\) are \textbf{independent},
then for any functions \(g, h: \mathbb{R}\rightarrow \mathbb{R}\), we have 
\[E[g(X)h(Y)] = E[g(X)]E[h(Y)].\]
\marker{Covariance of Independent} If \(X\) and \(Y\) are
\textbf{independent}, then \(\operatorname{Cov}\left(X,Y\right)= 0\).

\marker{Variance of a Sum}
\[\operatorname{Var}\left(\sum\limits_{k=1}^{n}X_k\right) =
\sum\limits_{k=1}^{n}\operatorname{Var}\left(X_k\right)+2\sum_{1 \leq i < j
\leq n} \operatorname{Cov}\left(X_i,X_j\right)\]
When \(X_1, X_2, \ldots, X_n\) are \textbf{independent}, we have
\[\operatorname{Var}\left(\sum\limits_{k=1}^{n}X_k\right) =
\sum\limits_{k=1}^{n}\operatorname{Var}\left(X_k\right).\]

\marker{Correlation Coefficient}: 
\(\rho_{X,Y} = \dfrac{\operatorname{Cov}\left(X,Y\right)}
{\sqrt{\operatorname{Var}\left(X\right)\operatorname{Var}\left(Y\right)}}\).
(Sometimes denoted by \(\operatorname{Corr}\))
\begin{multicols}{2}
\begin{itemize}
  \item \(-1 \leq \rho_{X,Y} \leq 1\);
  \item \(\rho\left(X,Y\right)= \pm 1\) \tf \(y = ax + b\).
\end{itemize}
\end{multicols}

\marker{Conditional Expectation}:
\begin{itemize}
  \item \textbf{D}: \(E\left[X\mid Y=y\right]=\sum\limits_{x}x\cdot p_{X\mid Y}\left(x\mid
y\right)\);
  \item \textbf{C}: \(E\left[X\mid Y=y\right]=\int_{-\infty}^{\infty}x\cdot f_{X\mid Y}\left(x\mid
y\right)\,\mathrm{d}x\).
\end{itemize}

\marker{Conditional Expectation of a Function}:
\begin{itemize}
  \item \textbf{D}: \(E[g(X)|Y=y] = \sum_x g(x)p_{X|Y}(x|y)\);
  \item \textbf{C}: \(E[g(X)|Y=y] = \int_{-\infty}^{\infty}
    g(x)f_{X|Y}(x|y)\,\mathrm{d}x\).
\end{itemize}
Therefore, \(E\left[\sum\limits_{k=1}^{n}X_k | Y=y\right] =
\sum\limits_{k=1}^{n}E[X_k|Y = y]\).

\marker{Law of Total Variance}:
\[\operatorname{Var}\left(X\right)=E\left[\operatorname{Var}\left(X\mid
Y\right)\right]+\operatorname{Var}\left(E\left[X\mid Y\right]\right).\]

\marker{Moment Generating Function (MGF)}:
\begin{itemize}
  \item \textbf{D}: \(M_X(t) := E[e^{tX}] = \sum_x e^{tx}p_X(x)\);
  \item \textbf{C}: \(M_X(t) := E[e^{tX}] = \int_{-\infty}^{\infty}
    e^{tx}f_X(x)\,\mathrm{d}x\).
\end{itemize}

\marker{Multiplicative Property of MGF} \(X\), \(Y\) are \textbf{independent}:
\[M_{X+Y}(t) = M_X(t)M_Y(t).\]
\marker{Uniqueness Property of MGF} Suppose that there exists an \(h < 0\) such
that
\(M_X(t) = M_Y(t), \quad \forall t \in (-h, h).\)
Then \(X\) and \(Y\) have the same distribution, i.e. \(F_X = F_Y\) and 
\(f_X = f_Y\).

\marker{Joint Moment Generating Function}:
\[M_{X_1, \cdots, X_n}(t_1, \cdots, t_n) = E\left[\exp({t_1X_1 + \cdots +
t_nX_n})\right].\]
\marker{Recover Individual MGF from Joint}:
\[M_{X_i}(t_i) = M_{X_1, \cdots, X_n}(0, \cdots, t_i, \cdots, 0).\]
\marker{Independence of Mean and Variance from Normal Sample} 
Let \(X_1, X_2, \ldots, X_n\) be independent and identically distributed normal
random variables with mean \(\mu\) and variance \(\sigma^2\). Then the sample
mean \(\overline{X}\) and the sample variance \(S^2\) are independent, and
\(\overline{X}\sim N\left(\mu, \sigma^2/n\right)\) and \((n-1)S^2/\sigma^2
\sim \chi^2(n-1)\).

