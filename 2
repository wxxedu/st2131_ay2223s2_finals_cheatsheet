\documentclass[10pt]{article}

\usepackage[margin=1cm]{geometry}
\geometry{a3paper}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{multicol}
\usepackage{xcolor}
\usepackage{mathtools}

% Water mark

% No Page Number
\pagenumbering{gobble}
% No Indent 
\setlength\parindent{0pt}
% Line Spacing to 1 
\renewcommand{\baselinestretch}{1.0}
% Remove line spacing before and after sections 
\usepackage{titlesec}
\titlespacing\section{0pt}{0pt plus 0pt minus 0pt}{0pt plus 0pt minus 0pt}
\titlespacing\subsection{0pt}{0pt plus 0pt minus 0pt}{0pt plus 0pt minus 0pt}
\titlespacing\subsubsection{0pt}{0pt plus 0pt minus 0pt}{0pt plus 0pt minus 0pt}

\renewcommand{\familydefault}{\sfdefault}

\AtBeginEnvironment{equation*}
{\setlength{\abovedisplayskip}{0pt}\setlength{\belowdisplayskip}{0pt}\color{violet}}
\AtBeginEnvironment{gather*}
{\setlength{\abovedisplayskip}{0pt}\setlength{\belowdisplayskip}{0pt}\color{violet}}
\AtBeginEnvironment{align*}
{\setlength{\abovedisplayskip}{0pt}\setlength{\belowdisplayskip}{0pt}\color{violet}}

% make sum, intersection, union, integral bigger
\let\smallsum\sum
\renewcommand{\sum}{\displaystyle\smallsum}
\let\smallprod\prod
\renewcommand{\prod}{\displaystyle\smallprod}
\let\oldint\int
\renewcommand{\int}{\displaystyle\oldint}
\let\oldbigcup\bigcup
\renewcommand{\bigcup}{\displaystyle\oldbigcup}
\let\oldbigcap\bigcap
\renewcommand{\bigcap}{\displaystyle\oldbigcap}

% Some other symbol
\newcommand{\tf}{$\Rightarrow$ }

% remove space before multicol 

\setlength{\multicolsep}{0pt}
% \setlength{\columnseprule}{0.2pt}
% \def\columnseprulecolor{\color{magenta}}

% reduce list spacing
\usepackage{enumitem}
\setlist{nosep}

% Colors


% Color tweaks

\newcommand{\marker}[1]{{\color{magenta}{\textbf{#1}}}}

% set section color to violet 
\let\oldsection\section
\renewcommand{\section}[1]{\oldsection{\color{violet}{#1}}}
\renewcommand{\thesection}{\color{violet}{\arabic{section}}}

\let\oldsubsection\subsection
\renewcommand{\subsection}[1]{\oldsubsection{\color{purple}{#1}}}
\renewcommand{\thesubsection}{\color{purple}{\arabic{section}.\arabic{subsection}}}

% Change color of equation to violet 
\let\oldinlinemathstart\(
\renewcommand{\(}{\begingroup\color{violet}\oldinlinemathstart}
\let\oldinlinemathend\)
\renewcommand{\)}{\oldinlinemathend\endgroup}

% Itemize 

\usepackage{enumitem}
\setlist[itemize]{
  before=\small,
  label=\tiny\textcolor{magenta}{\textbullet},
  leftmargin=*,labelsep=0.3em
}
\setlist[enumerate]{
  before=\small,
  label=\small\textcolor{magenta}{\arabic*.},
  leftmargin=*,labelsep=0.3em
}


\begin{document}
% \begin{center}
% \LARGE ST2131 Finals Cheatsheet by Wang Xiuxuan
% \end{center}
\begin{multicols*}{3}
\section{Chapter 1}

\marker{Basic Principle of Counting}: \(A\): \(m\) outcomes, \(B\): \(n\)
outcomes, \tf \(A\cap B\): \(mn\) outcomes.
\begin{itemize}
  \item \textbf{Generalized}: \(A_i\): \(i\) outcomes \tf \(\smallprod n_i\)
    outcomes.
\end{itemize}

\marker{Permutations} \(n\) distinct objs \tf \(n!\) permutations.
\begin{itemize}
  \item \textbf{Generalized}: \(n = \sum n_i\), \(n_i\) distinct \tf 
    \({n!}/{\left(\smallprod n_i!\right)}\) permutations.
\end{itemize}

\marker{Combinations} \(n\) distinct choose \(r\) \tf 
\(\binom{n}{r}=n!/r!(n-r)!\).
\begin{multicols}{2}
\begin{itemize}
  \item \(r < 0\) or \(r > n\) \tf \(\binom{n}{r} = 0\);
  \item \(\binom{n}{r}=\binom{n-1}{r-1}+\binom{n-1}{r}\).
\end{itemize}
\end{multicols}

\marker{Binomial Thm} \((x+y)^n = \sum_{k=0}^n \binom{n}{k}x^ky^{n-k}\).
\begin{multicols}{2}
\begin{itemize}
  \item \(\sum\limits_{k=0}^{n}\binom{n}{k}=2^n\);
  \item \(\sum\limits_{k=0}^{n}(-1)^k\binom{n}{k}=0\), \(n \geq 1\);
\end{itemize} 
\end{multicols}
\begin{itemize}
  \item \(\binom{n}{0}+\binom{n}{2}+\cdots =
    \binom{n}{1}+\binom{n}{3}+\cdots\).
\end{itemize}

\marker{Multinomial Coef} \(n = \smallsum n_i\), \(n_i\) distinct, choose \(r\)
groups \tf \(\binom{n}{n_1, n_2, \cdots, n_r} = {n!}/{\left(\smallprod
  n_i!\right)}\).
\begin{itemize}
  \item \(\left(\sum\limits_{i=1}^{r}x_i\right)=\sum_{n_1 + n_2 + \cdots + n_r =
    n}\binom{n}{n_1,n_2,\cdots,n_r}\prod\limits_{i=1}^{r}x_i^{n_i}\)
\end{itemize}

\marker{Integer Sols} Solve \(x_1 + x_2 + \cdots + x_r = n\), (\(x_i > 0\),
\(\forall i\))
\begin{multicols}{2}
\begin{itemize}
  \item \(\binom{n-1}{r-1}\) positive int sols;
  \item \(\binom{n+r-1}{r-1}\) non-negative int sols.
\end{itemize}
\end{multicols}

\section{Chapter 2}
\marker{Set Ops} Laws:
\begin{itemize}
  \item \textbf{Commutative} \(EF = FE\), \(E\cup F = F\cup E\);
  \item \textbf{Associative} \((EF)G = E(FG)\), \((E\cup F)\cup G = E\cup
    (F\cup G)\);
  \item \textbf{Distributive} \(E(F\cup G) = EF\cup EG\), \(E\cap(F\cup G) =
    (E\cap F)\cup(E\cap G)\);
  \item \textbf{DeMorgan} \((\bigcup_{i=1}^n E_i)^c = \bigcap_{i=1}^n E_i^c\),
    \((\bigcap_{i=1}^n E_i)^c = \bigcup_{i=1}^n E_i^c\).
\end{itemize}

\marker{Axioms of Probability}  
\begin{itemize}
  \item \(0 \leq P(E)\leq 1\); 
  \item \(S:=\text{sample space}\) \tf \(P(S) = 1\); 
  \item \(E_1, E_2, \cdots\) disjoint \tf \(P\left(\bigcup_{i=1}^\infty
      E_i\right) = \sum_{i=1}^\infty P(E_i)\).
\end{itemize}

\marker{Properties of Probability}
\begin{multicols}{2}
\begin{itemize}
  \item \(P(\emptyset) = 0\);
  \item \(P\left(\bigcup_{i=1}^n E_i\right) = \sum\limits_{i=1}^{n}P(E_i)\);
  \item \(P(E^c)=1-P(E)\)
  \item \(P(A) \leq P(B)\) if \(A \subseteq B\);
  \item \( \begin{aligned} P(A\cup B) =& P(A) + P(B) \\ 
           &- P(A\cap B)\end{aligned} \)
\end{itemize} 
\end{multicols}

\marker{Inclusion Exclusion} 
\begin{align*}
   &P(E_1\cup E_2\cup \cdots \cup E_n) \\
  =&\sum\limits_{i=1}^{n}P(E_i)-\sum\limits_{\mathclap{i\leq i_1 < i_2 \leq n}}
  P(E_{i_1}\cap E_{i_2}) + \cdots + (-1)^{r+1} \\
   &+ (-1)^{n-1}P(E_1\cap E_2\cap \cdots \cap E_n)
\end{align*}

\marker{\(\nearrow\), \(\searrow\) Sequences} \(\{E_n\}\) is
\textbf{increasing} if \(E_1 \subseteq E_2 \subseteq \cdots\),
\textbf{decreasing} if \(E_1 \supseteq E_2 \supseteq \cdots\).
\begin{multicols}{2}
\begin{itemize}
  \item \(\{E_n\} \nearrow\) \tf \(\lim\limits_{\mathclap{n\rightarrow \infty}}E_n =
    \bigcup\limits_{i=1}^{\infty}E_i\);
  \item \(\{E_n\} \searrow\) \tf \(\lim\limits_{\mathclap{n\rightarrow \infty}}E_n =
    \bigcap\limits_{i=1}^{\infty}E_i\).
\end{itemize}
\end{multicols}

\section{Chapter 3}

\marker{Conditional Probability} \(A\) happens given \(B\) happens:
\[P(A|B) = \frac{P(A\cap B)}{P(B)}.\]
\begin{itemize}
  \item \(P(AB) := P(A\cap B) = P(A|B)P(B)\) (multiplicative rule);
  \item \textbf{General}: \(P(A_1..A_2)=P(A_1)P(A_2|A_1)..
    P(A_n|A_1A_2\cdots A_{n-1})\)
\end{itemize}

\section{Chapter 4}
\subsection{Random Variables}
\marker{Definition 4.1}: A \textbf{random variable}, \(X\), is a mapping from the
sample space to real numbers.
\subsection{Discrete Random Variables}

\marker{Definition 4.2}: A random variable is said to be \textbf{discrete} if 
range of \(X\) is either \textbf{fininite} or \textbf{countably infinite}.

\marker{Definition 4.3}: The \textbf{probability mass function} (pmf) of a
discrete random variable \(X\) is defined by
\begin{equation*}
p_X(x) = \begin{cases}P(X=x), & \text{if }x = x_1, x_2, \cdots \\ 0, & 
\text{otherwise}.\end{cases}
\end{equation*}
Note that \(\sum\limits_{i=1}^{\infty}p_X(x_i) = 1.\)

\marker{Definition 4.4}: The \textbf{cumulative distribution function} (cdf) of
a discrete random variable \(X\) is defined as 
\(F_X:\mathbb{R}\rightarrow \mathbb{R}T\) where \(F_X(x) = P(X\leq x)\), for 
\(x\in \mathbb{R}.\)

\subsection{Expected Value}

\marker{Definition 4.5}: If \(X\) is a discrete random variable having a 
probability mass function \(p_X\), the \textbf{expectation} or \textbf{expected 
value} of \(X\) is defined by 
\begin{equation*}
  E(X) = \sum\limits_{x}xp_X(x).
\end{equation*}

\marker{Tail Sum Formula}: For \textbf{nonnegative integer-valued} random 
variable \(X\), we have 
\begin{equation*}
E(X) = \sum\limits_{k=1}^{\infty}P(X \geq k) = 
\sum\limits_{k=0}^{\infty}P(X > k).
\end{equation*}

\subsection{Expectation of a Function of a Random Variable}

\marker{Proposition 4.1} If \(X\) is a \textbf{discrete random variable} that 
takes values \(x_i\), \(i \geq 1\), with respective probabilities \(p_X(x_i)\), 
then for any real value function \(g\), 
\begin{align*}
  E[g(X)] &= \sum_i g(x_i)p_X(x_i) \quad \text{or equivalently}\\
          &= \sum_x g(x)p_X(x).
\end{align*}

\marker{Corollary 4.2} Let \(a\) and \(b\) be constants, then 
\begin{equation*}
E[aX + b] = aE(X)+ b.
\end{equation*}

\subsection{Variance and Standard Deviation}

\marker{Definition 4.6} If \(X\) is a random variable with mean \(\mu\), then the 
\textbf{variance} of \(X\), denoted by \(\operatorname{Var}\left(X\right)\), is 
defined by 
\begin{equation*}
  \operatorname{Var}\left(X\right)=E[(X-\mu)^2].
\end{equation*}

\marker{Definition 4.7} The \textbf{standard deviation} of \(X\), denoted by 
\(\sigma_X\) or \(\operatorname{SD}\left(X\right)\), is defined by
\begin{equation*}
\sigma_X = \sqrt{\operatorname{Var}\left(X\right)}.
\end{equation*}

\marker{Scaling and shifting}: 
\begin{itemize}
  \item \(\operatorname{Var}\left(aX+b\right)=a^2\operatorname{Var}
    \left(X\right).\)
  \item \(\operatorname{SD}\left(aX+b\right)=|a|\operatorname{SD}
    \left(X\right).\)
\end{itemize}


\subsection{Discrete Random Variables Arising from Repeated Trials}

\marker{Bernoulli Random Varaible} \(\operatorname{Be}\left(p\right)\), define 
\begin{equation*}
  X = \begin{cases}1, &\text{if it is a success};\\ 
  0 &\text{if it is a failure.}\end{cases}
\end{equation*}
\begin{multicols}{2}
\begin{itemize}
\item \(P(X=1) = p\); 
\item \(P(X=0) = 1-p\);
\item \(E(X) = p\);
\item \(\operatorname{Var}\left(X\right)=p(1-p)\).
\end{itemize} 
\end{multicols}

\marker{Binomial Random Varaible} \(\operatorname{Bin}\left(n,p\right)\), define
\(X\) as \textbf{number of successes} in \(n\)
\(\operatorname{Bernoulli}\left(p\right)\) trials.

Therefore, \(X\) takes values \(0, 1, 2, \cdots, n\). 
\begin{itemize}
  \item \(P(X=k)=\binom{n}{k}p^k(1-p)^{n-k}\);
\end{itemize}
\begin{multicols}{2}
\begin{itemize}
  \item \(E(X)=np\);
  \item \(\operatorname{Var}\left(X\right)=np(1-p)\);
\end{itemize}
\end{multicols}

\marker{Geometric Random Varaible} \(\operatorname{Geom}\left(p\right)\), 
define \(X\) as \textbf{number of trials} until the first success in a 
sequence of independent \(\operatorname{Bernoulli}\left(p\right)\) trials.
\begin{itemize}
  \item \(P(X=k) = pq^{k-1}\);
\end{itemize}
\begin{multicols}{2}
\begin{itemize}
  \item \(E(X)=\dfrac{1}{p}\);
  \item \(\operatorname{Var}\left(X\right)=\dfrac{1 - p}{p^2}\);
\end{itemize}
\end{multicols}

\marker{Alternative Geometric Distribution}: Let \(X'\) be the 
\textbf{number of failures} in the \(\operatorname{Bernoulli}\left(p\right)\)
trials in order to obtain the first success. Here, 
\begin{equation*}
  X = X' + 1
\end{equation*}
\begin{itemize}
  \item \(P(X'=k) = pq^{k}\);
\end{itemize}
\begin{multicols}{2}
\begin{itemize}
  \item \(E(X')=\dfrac{1-p}{p}\);
  \item \(\operatorname{Var}\left(X'\right)=\dfrac{1 - p}{p^2}\);
\end{itemize}
\end{multicols}

\marker{Negative Binomial Random Varaible} \(\operatorname{NB}
\left(r,p\right)\), define \(X\) to be \textbf{number of
\(\operatorname{Bernoulli}\left(p\right)\) trials} required to obtain 
\textbf{$r$ successes}. For \(k \geq r\), we have:
\begin{itemize}
  \item \(P(X=k) = \binom{k-1}{r-1}p^r(1-p)^{k-r}\);
\end{itemize}
\begin{multicols}{2}
\begin{itemize}
  \item \(E(X)=\dfrac{r}{p}\);
  \item \(\operatorname{Var}\left(X\right)=\dfrac{r(1-p)}{p^2}\);
\end{itemize}
\end{multicols}

\marker{Remark}: \(\operatorname{Geom}\left(p\right)=
\operatorname{NB}\left(1,p\right).\)

\subsection{Poisson Random Variable}

\marker{Poisson}: A random variable \(X\) is said to have a \textbf{Poisson} 
distribution with parameter \(\lambda\) if \(X\) takes values 
\(0, 1, 2, \cdots\) with probabilities given as:
\begin{equation*}
  P(X = k) = \dfrac{e^{-\lambda}\lambda^k}{k!}, \quad \text{for }k=0, 1, 2, 
  \cdots
\end{equation*}
\begin{multicols}{2}
\begin{itemize}
  \item \(E(X)=\lambda\);
  \item \(\operatorname{Var}\left(X\right)=\lambda\).
\end{itemize}
\end{multicols}
When \(n\) is large and \(\lambda\) is moderate, we can use \textbf{poisson} to
approximate \textbf{binomial}:
\begin{equation*}
  \operatorname{Bin}\left(n,p\right) \approx \operatorname{Poisson}
  \left(np\right).
\end{equation*}
I.e. \(P(X = k)\approx e^{-\lambda}\dfrac{\lambda^k}{k!}\).

\marker{Examples obeying Poisson distribution}:
\begin{itemize}
  \item Number of misprints on a page;
  \item Number of people in a community living to 100 years;
  \item Number of wrong telephone numbers that are dialled in a day;
  \item Number of people entering a store on a given day;
  \item Number of particles emitted by a radioactive source.
\end{itemize}
These are approximately Poisson \textbf{because of the Poisson approximation 
to the binomial distribution.}

\subsection{Hypergeometric Random Variable}

\marker{Hypergeometric}: A random variable \(X\) is said to have a
\textbf{hypergeometric} distribution with parameters \(N\), \(m\) and \(n\) if
\(X\) takes values \(0, 1, 2, \cdots, n\) with probabilities given as:
\begin{equation*}
  P(X = x) = \dfrac{\binom{m}{x}\binom{N-m}{n-x}}{\binom{N}{n}}
\end{equation*}
for \(x = 0, 1, \cdots, \min(m, n)\).
\begin{itemize}
  \item \(\operatorname{E}\left(X\right)=\dfrac{nm}{N}\);
  \item \(\operatorname{Var}\left(X\right)=\dfrac{nm}{N}\left[
    \dfrac{(n-1)(m-1)}{N-1}+1 - \dfrac{nm}{N}
  \right]\).
\end{itemize}

\marker{Example}: Suppose that we have $N$ balls, of which $m$ are red and 
$N-m$ are blue. We choose $n$ of these balls, \textbf{without replacement},
and define $X$ to be the number of red balls chosen. Then $X$ is a
\textbf{hypergeometric} random variable, with 
\(P(X = x) = {\binom{m}{x}\binom{N-m}{n-x}} / {\binom{N}{n}}\).

\subsection{Distribution Functions and Probability Mass Functions}

\marker{Properties of DF}:
\begin{itemize}
  \item \(F_X\) is a \textbf{non-decreasing} function, i.e. if $a < b$, then 
  \(F_X(a) \leq F_X(b)\);
\item \(\lim\limits_{b\rightarrow \infty}F_X(b) = 1\), and
  \(\lim\limits_{b\rightarrow -\infty}F_X(b) = 0\)
\item \(F_X\) has \textbf{left limits}, i.e. \(\lim\limits_{x\rightarrow
  b^-}F_X(x)\) exists for all \(b\in \mathbb{R}\).
\item \(F_X\) is \textbf{right continuous}, that is for any \(b \in
  \mathbb{R}\), \(\lim\limits_{x\rightarrow b^+}F_X(x) = F_X(b)\).
\end{itemize}

\marker{Useful Calculations}:
\begin{enumerate}
  \item Calculating probabilities from \textbf{DF}:
  \begin{itemize}
    \item \(P(a<X\leq b)=F_X(b) - F_X(a)\);
    \item \(P(X=a)=F_X(a)-F_X(a^-)\), where 
      \(F_X(a^-)=\lim\limits_{x\rightarrow a^-}F_X(x)\).
    \item \(P(a \leq X \leq b) = P(X = a) + P(a < X \leq b)=F_X(b)-F_X(a^-)\).
  \end{itemize}
  \item Calculating probabilities from \textbf{PMF}:
    \[P(A) = \sum_{x\in A}p_X(x).\]
  \item Calculating \textbf{PMF} from \textbf{DF}:
    \[p_X(x) = F_X(x) - F_X(x^-), \quad x \in \mathbb{R}.\]
  \item Calculating \textbf{DF} from \textbf{PMF}:
    \[F_X(x) = \sum_{y\leq x}p_X(y), \quad x \in \mathbb{R}.\]
\end{enumerate}

\section{Continuous Random Variables}

\subsection{Introduction}

\marker{Definition 5.1} We say that \(X\) is a \textbf{continuous} random
variable if there exists a nonnegative function $f_X$ defined for all real 
$x \in \mathbb{R}$ such that 
\[P(a < X \leq b) = \int_a^b f_X(x)dx,\]
for \(-\infty < a < b < +\infty\). The function $f_X$ is called the
\textbf{probability density function} (p.d.f.) of $X$.

\marker{Definition 5.2} We define the \textbf{distribution function} of \(X\)
by \[F_X(x) = P(X \leq x),\]for \(x \in \mathbb{R}\).

\marker{Remark}: since definition of \textbf{DF} is the same for both
discrete and continuous random variables:

\(F_X(x) = \int_{-\infty}^{x}f_X(t)\ \mathrm{d}t\), and
\(f_X(x) = \dfrac{\partial{d}}{\partial{d}x}F_X(x)\).
Therefore, we have:
\begin{itemize}
  \item $P(X = x) = 0$;
  \item The distribution function, $F_X(x)$, is continuous;
  \item For any $a, b \in (-\infty, \infty)$, 
    \begin{align*}
      P(a \leq X \leq b) &= P(a < X \leq b) \\ 
                         &= P(a \leq X < b) \\ 
                         &= P(a < X < b).
    \end{align*}
\end{itemize}

\marker{Determining constant}: we have: 
\[1 = P(-\infty < X < \infty) = \int_{-\infty}^{\infty}f_X(x)\ \mathrm{d}x\]

\subsection{Expectation and Variance}

\marker{Expectation}: For a continuous random variable \(X\) with p.d.f.
\(f_X(x)\), we define the \textbf{expectation} of \(X\) by
\[E(X) = \int_{-\infty}^{\infty}xf_X(x)\ \mathrm{d}x,\]
and the \textbf{variance} of \(X\) by
\[\operatorname{Var}(X) = \int_{-\infty}^{\infty}(x-E(X))^2f_X(x)\ \mathrm{d}x.\]

\marker{Proposition 5.1} If \(X\) is a \textbf{continuous random variable} 
with \textbf{p.d.f.} \(f_X(x)\), then for any real value function \(g(x)\),
\begin{itemize}
  \item \(E(g(X)) = \int_{-\infty}^{\infty}g(x)f_X(x)\ \mathrm{d}x\);
  \item \(E(aX + b)\) = \(aE(X) + b\);
  \item \(\operatorname{Var}(X) = E(X^2) - [E(X)]^2\).
\end{itemize}

\marker{Lemma 5.1 Tail Sum Formula} Let \(X\) be a \textbf{nonnegative}
random variable, then
\[E(X) = \int_0^{\infty}P(X > x)\ \mathrm{d}x.\]

\subsection{Uniform Distribution}

\marker{Definition} A continuous random variable \(X\) is said to have a
\textbf{uniform distribution} on the interval \((a, b)\) if its p.d.f. is given
by
\[f_X(x) = \begin{cases}
  \dfrac{1}{b-a}, & a < x < b \\
  0, & \text{otherwise}.
\end{cases}\]
Then, \[F_X(x) = \begin{cases}
  0, & x < a \\
  \dfrac{x-a}{b-a}, & a \leq x < b \\
  1, & b \leq x.
\end{cases}\]
We have:
\begin{multicols}{2}
  \begin{itemize}
    \item \(E(X) = \dfrac{a+b}{2}\);
    \item \(\operatorname{Var}(X) = \dfrac{(b-a)^2}{12}\).
  \end{itemize}
\end{multicols}

\subsection{Normal Distribution}

\marker{Definition} A continuous random variable \(X\) is said to have a
\textbf{normal distribution} with parameters \(\mu\) and \(\sigma^2\) if its
p.d.f. is given by
\[f_X(x) = \dfrac{1}{\sqrt{2\pi}\sigma}e^{-{(x-\mu)^2}/{2\sigma^2}},\]
for \(-\infty < x < \infty\). We write \(X \sim N(\mu, \sigma^2)\).

\marker{Standard Normal} A continuous random variable \(Z\) is said to have a
\textbf{standard normal distribution} if its p.d.f. is given by
\[f_Z(z) = \dfrac{1}{\sqrt{2\pi}}e^{-{z^2}/{2}},\]
for \(-\infty < z < \infty\). We write \(Z \sim N(0, 1)\).

If \(X \sim N(\mu, \sigma^2)\), then
\(Z = \dfrac{X-\mu}{\sigma} \sim N(0, 1),\)
and 
\begin{align*}
P(a < Y \leq b) &= P\left(\dfrac{a-\mu}{\sigma}< Z \leq
  \dfrac{b-\mu}{\sigma}\right) \\ 
                &= \Phi\left(\dfrac{b-\mu}{\sigma}\right) - 
                \Phi\left(\dfrac{a-\mu}{\sigma}\right).
\end{align*}
\marker{Properties of Normal Distribution}
\begin{itemize}
  \item \(P(Z \geq 0) = P(Z \leq 0) = \dfrac{1}{2}\);
  \item \(-Z \sim N(0, 1)\);
  \item \(P(Z \leq x) = 1 - P(Z > x)\) for \(-\infty < x < \infty\);
  \item \(P(Z \leq -x) = P(Z \geq x)\) for \(-\infty < x < \infty\);
  \item if \(Y \sim N(\mu, \sigma^2)\), then \(X := (Y-\mu)/\sigma \sim N(0,
    1)\);
  \item if \(X \sim N(0, 1)\), then \(Y := aX + b \sim N(b, a^2)\).
\end{itemize}

\marker{Expectation and Variance}
\begin{itemize}
  \item if \(Y \sim N(\mu, \sigma^2)\), then \(E(Y) = \mu\) and \(\operatorname{Var}(Y) =
    \sigma^2\);
  \item if \(Z\sim N(0,1)\),  then \(E(Z)=0\) and \(\operatorname{Var}(Z)=1\).
\end{itemize}

\subsection{Expontential Distribution}

\marker{Definition} A continuous random variable \(X\) is said
to have an \textbf{exponential distribution} with parameter \(c\), 
\(\lambda > 0\) if its \textbf{probability density function} is given by
\[f_X(x) = \begin{cases}
  ce^{-\lambda x}, & x > 0 \\
  0, & x \leq 0.
\end{cases}\]
Its \textbf{distribution function} is given by
\[F_X(x) = \begin{cases}
  1 - e^{-\lambda x}, & x > 0 \\
  0, & x \leq 0.
\end{cases}\]
\begin{itemize}
  \item \textbf{Memoryless Property}: 
    \[P(X > s + t | X > s) = P(X > t), \quad s, t > 0.\]
\end{itemize}
\begin{multicols}{2}
  \begin{itemize}
    \item \(E(X) = \dfrac{1}{\lambda}\);
    \item \(\operatorname{Var}(X) = \dfrac{1}{\lambda^2}\).
  \end{itemize} 
\end{multicols}

\subsection{Some Other Distributions}

\marker{Gamma} A continuous random variable \(X\) is said to have a
\textbf{gamma distribution} with parameters \(\alpha\) and \(\lambda\), where
\(\alpha > 0\) and \(\lambda > 0\), if its \textbf{probability density function}
is given by
\[f_X(x) = \begin{cases}
  \dfrac{\lambda^{\alpha}x^{\alpha-1}e^{-\lambda x}}{\Gamma(\alpha)}, & x > 0 \\
  0, & x \leq 0,
\end{cases}\]
where \(\Gamma(\alpha) = \int_0^{\infty} x^{\alpha-1}e^{-x}dx\).
Its \textbf{distribution function} is given by
\[F_X(x) = \begin{cases}
  \dfrac{\gamma(\alpha, \lambda x)}{\Gamma(\alpha)}, & x > 0 \\
  0, & x \leq 0.
\end{cases}\]
\begin{multicols}{2}
  \begin{itemize}
    \item \(E(X) = \dfrac{\alpha}{\lambda}\);
    \item \(\operatorname{Var}(X) = \dfrac{\alpha}{\lambda^2}\).
    \item \(\Gamma(1) = \int_{0}^{\infty}e^{-y}\ \mathrm{d}y = 1.\)
    \item \(\Gamma(\alpha) = (\alpha - 1)\Gamma(\alpha - 1)\).
    \item \(\Gamma(n) = (n-1)!\) for \(n \in \mathbb{N}\).
  \end{itemize}
\end{multicols}

\marker{Weibull} A continuous random variable \(X\) is said to have a
\textbf{Weibull distribution} with parameters \(\alpha\) and \(\lambda\), where
\(\alpha > 0\) and \(\lambda > 0\), if its \textbf{probability density function}
is given by \(f_X(x) =\)
\[\begin{cases}
  \dfrac{\beta}{\alpha}\left(\dfrac{x-v}{\alpha}\right)^{\beta -
    1}\operatorname{exp}\left[-\left(\dfrac{x-v}{\alpha}\right)^\beta \right],
    & x > v \\
  0, & x \leq v,
\end{cases}\]
If \(X \sim \operatorname{W}\left(v,\alpha,\beta\right)\), then 
\begin{itemize}
  \item \(\operatorname{E}(X) = v + \alpha\Gamma\left(1 + \dfrac{1}{\beta}\right)\);
  \item \(\operatorname{Var}(X) = \alpha^2\left[\Gamma\left(1 + \dfrac{2}{\beta}\right) -
    \left(\Gamma\left(1 + \dfrac{1}{\beta}\right)\right)^2\right]\).
\end{itemize}

\marker{Cauchy} A continuous random variable \(X\) is said to have a
\textbf{Cauchy distribution} with parameters \(\alpha\) and \(\beta\), where
\(\theta \in \mathbb{R}\) and \(\alpha > 0\), if its \textbf{probability density
function} is given by
\[f_X(x) = \dfrac{1}{\pi\alpha\left[1 + \left(\dfrac{x-\theta}{\alpha}\right)^2\right]},\]
\begin{itemize}
  \item \(E(X)\) does not exist;
  \item \(\operatorname{Var}(X)\) does not exist.
\end{itemize}

\marker{Beta} A continuous random variable \(X\) is said to have a
\textbf{beta distribution} with parameters \(\alpha\) and \(\beta\), where
\(\alpha > 0\) and \(\beta > 0\), if its \textbf{probability density function}
is given by
\[f_X(x) = \begin{cases}
  \dfrac{1}{B(a,b)}x^{\alpha-1}(1-x)^{\beta-1},
  & 0 < x < 1 \\
  0, & \text{otherwise}.
\end{cases}\]
where \(B(\alpha, \beta) = \int_0^1 x^{\alpha-1}(1-x)^{\beta-1}dx\), is the 
\textbf{Beta function}. 
\begin{itemize}
  \item \(E(X) = \dfrac{\alpha}{\alpha + \beta}\);
  \item \(\operatorname{Var}(X) = \dfrac{\alpha\beta}{(\alpha + \beta)^2(\alpha +
    \beta + 1)}\).
\end{itemize}

\subsection{Approximation of Binomial Distribution}

\marker{Theorem 5.1 (De Moivre-Laplace Theorem)} Suppose that \(X \sim
\operatorname{Bin}\left(n,p\right)\). Then, for any \(a < b\), we have
\[P\left(a\leq \dfrac{X-np}{\sqrt{npa}} \leq b\right) \rightarrow \Phi(b) -
\Phi(a)\]
as \(n \rightarrow \infty\), where \(q = 1-p\) and \(\Phi(z) = P(Z \leq z)\) 
with \(Z \sim \operatorname{N}\left(0,1\right)\). That is, 
\[\operatorname{Bin}\left(n,p\right)\approx
\operatorname{N}\left(np,npq\right).\]
Equivalently, \(\dfrac{X-np}{\sqrt{npq}}\approx Z\) where \(Z \sim
\operatorname{N}\left(0,1\right)\).

Generally good for values of \(n\) satisfying \(npq \geq 10\).

\marker{Continuity Correction} If \(X \sim \operatorname{Bin}\left(n,p\right)\),
then 
\begin{align*}
P(X=k) &= P(k-0.5 < X < k + 0.5) \\
P(X \geq k) &= P(X \geq k- 0.5) \\ 
P(X \leq k) &= P(X \leq k + 0.5)
\end{align*}

\marker{Poisson Approximation to Binomial} Works when \(n\) is large and \(p\) is
small. \textbf{As a working rule, $p < 0.1$ and set $\lambda = np$}.

\subsection{Distribution of a Function of a Random Variable}

\marker{$\chi^2$-Distribution} Let \(X_1, X_2, \ldots, X_n\) be independent
standard normal random variables. Then, the random variable
\[Q = \sum_{i=1}^{n}X_i^2\]
has a \textbf{chi-squared distribution} with \(n\) degrees of freedom, denoted
by \(Q \sim \chi^2(n)\).
The \textbf{p.d.f.} of \(Q\) is given by
\[f_Q(q) = \begin{cases}
  \dfrac{1}{2^{n/2}\Gamma(n/2)}q^{n/2-1}e^{-q/2}, & q > 0 \\
  0, & q \leq 0.
\end{cases}\]
The \textbf{d.f.} of \(Q\) is given by
\[F_Q(q) = \begin{cases}
  \dfrac{\gamma(n/2,q/2)}{\Gamma(n/2)}, & q > 0 \\
  0, & q \leq 0,
\end{cases}\]
where \(\gamma = \int_{0}^{q/2}t^{n/2-1}e^{-t}\,dt\) and
\(\Gamma = \int_{0}^{\infty}t^{n/2-1}e^{-t}\,dt\).
\begin{multicols}{2} 
\begin{itemize}
  \item \(E(Q) = n\);
  \item \(\operatorname{Var}(Q) = 2n\).
\end{itemize}
\end{multicols}

\marker{Log-Normal Distribution} Let $Z$ be a standard normal random variable.
Then, the random variable $X = e^{\mu + \sigma Z}$ has a \textbf{log-normal
distribution} with parameters $\mu$ and $\sigma$, denoted by $X \sim
\operatorname{LN}(\mu, \sigma)$. The \textbf{p.d.f.} of $X$ is given by
\[f_X(x) = \begin{cases}
  \dfrac{1}{x\sigma\sqrt{2\pi}}\exp\left[-\frac{1}{2\sigma^2}(\ln x - \mu)^2 
  \right], & x > 0 \\
  0, & x \leq 0.
\end{cases}\]
The \textbf{d.f.} of $X$ is given by
\[F_X(x) = \begin{cases}
  \dfrac{1}{2} + \dfrac{1}{2}\operatorname{erf}\left(\dfrac{\ln x - \mu}{\sigma\sqrt{2}}\right), & x > 0 \\
  0, & x \leq 0.
\end{cases}\]
where \(\operatorname{erf}(z) = \dfrac{2}{\sqrt{\pi}}\int_{0}^{z}e^{-t^2}\,dt\)
is the \textbf{error function}.

\marker{Theorem 5.2} Let \(X\) be a \textbf{continuous} random variable having
probaility density function \(f_X\). Suppose that \(g(x)\) is a
\textbf{strictly monotonic}, \textbf{differentiable} function of \(X\). Then
random variable \(Y\) defined by \(Y = g(X)\) has a \textbf{probability density
function} given by
\[f_Y(y) = \begin{cases}
  f_X(g^{-1}(y))\left|\dfrac{d}{dy}g^{-1}(y)\right|, & \text{if } y \in
  g(\mathcal{R}_X) \\
  0, & \text{otherwise}.
\end{cases}\]
where \(g^{-1}\) is the inverse function of \(g\).


\section{Jointly Distributed Random Variables}

\subsection{Joint Distribution Functions}

\marker{Definition 6.1} For any two random variables \(X\) adn \(Y\) defined on
the same sample space, we define the \textbf{joint distribution function} of 
\(X\) and \(Y\), denoted by \(F_{XY}(x,y)\), by
\[F_{XY}(x,y) = P(X \leq x, Y \leq y), \quad \text{for }x,y\in \mathbb{R},\]
where \(\{X \leq x, Y \leq y\} := \{X \leq x\}\cap \{Y \leq y\}\).

The \textbf{Marginal Distribution Function} of \(X\) and \(Y\) are:
\begin{align*}
  F_X(x) &= P(X \leq x) = P(X \leq x, Y < \infty) = F_{XY}(x,\infty) \\
  F_Y(y) &= P(Y \leq y) = P(X < \infty, Y \leq y) = F_{XY}(\infty,y)
\end{align*}

\marker{Useful Calculations}
\begin{align*}
   & P(X > a, Y > b) \\
  =& 1 - F_X(a) - F_Y(b) + F_{XY}(a,b); \\
   & P(a_1 < X \leq a_2, b_1 < Y \leq b_2) \\
  =& F_{XY}(a_2,b_2) - F_{XY}(a_1,b_2) 
   - F_{XY}(a_2,b_1) + F_{XY}(a_1,b_1)
\end{align*}

\marker{Joint Discrete Random Variables} The \textbf{joint pmf} of $X$ and $Y$,
denoted by $p_{X, Y}(x, y)$, is defined by
\[p_{X, Y}(x, y) = P(X = x, Y = y), \quad \text{for }x,y\in \mathbb{R}.\]
We can recover the \textbf{marginal p.m.f} of $X$ and $Y$ by
\begin{align*}
  p_X(x) = \sum_{y}p_{X, Y}(x, y),\quad
  p_Y(y) = \sum_{x}p_{X, Y}(x, y).
\end{align*}
\marker{Some useful formulas}
\begin{itemize}
  \item \(P(a_q < X \leq a_2,b_1 < Y \leq b_2) = \sum_{a_1 < x \leq a_2}
    \sum_{b_1 < y \leq b_2}p_{X, Y}(x, y)\);
  \item \(F_{X, Y}(a, b) = P(X \leq a, Y \leq b) = \sum_{x\leq a}\sum_{y \leq
    b} p_{(X, Y)}(x, y)\);
  \item \(P(X > a, Y > b) = \sum_{x > a}\sum_{y > b}p_{(X, Y)}(x,y)\).
\end{itemize}

\marker{Jointly Continuous Random Variables} We say that \(X\) and \(Y\) are
\textbf{jointly continuous random variables} if there exists a function 
\(f_{X, Y}(x, y)\), called the \textbf{joint probability density function} of
\(X\) and \(Y\), defined for all real \(x\) and \(y\), having the property that
for every set $C$ of paris of real numbers, (i.e. \(C \subset \mathbb{R}^2\)),
we have 
\[P\left((X, Y)\in C\right):=\int\int_{(x,y)\in
C}f_{X,Y}(x,y)\,\mathrm{d}x\mathrm{d}y.\]
We can recover the \textbf{marginal p.d.f.} of \(X\) and \(Y\): 
\(f_X(x)=\int_{-\infty}^{\infty}f_{X, Y}(x,y)\ \mathrm{d}y\) and 
\(f_Y(y)=\int_{-\infty}^{\infty}f_{X, Y}(x, y)\ \mathrm{d}x\).

\marker{Some useful formulas}
\begin{itemize}
  \item Let \(A, B \subset \mathbb{R}\), take \(C = A \times B\) above 
    \[P(X \in A, Y \in B) = \int_A\int_B
    f_{X,Y}(x,y)\,\mathrm{d}y\mathrm{d}x.\]
  \item Let \(a_1, a_2, b_1, b_2 \in \mathbb{R}\) where \(a_1 < a_2\) and \(b_1
    < b_2\), we have 
    \[P(a_1 < X \leq a_2, b_1 < Y \leq b_2) = \int_{a_1}^{a_2}\int_{b_1}^{b_2}
    f_{X, Y}(x,y)\,\mathrm{d}y\mathrm{d}x.\]
  \item Let \(a, b\in \mathbb{R}\), we have
    \[F_{X, Y}(a,b) = P(X \leq a, Y \leq b) = \int_{-\infty}^a\int_{-\infty}^b
      f_{X, Y}(x, y)\,\mathrm{d}y\mathrm{d}x.\]
  \item As a result:
    \[f_{X, Y}(x, y)\dfrac{\partial^2}{\partial x \partial y}F_{X, Y}(x,y).\]
\end{itemize}

\subsection{Independent Random Variables}

\marker{Definition} Two random variables \(X\) and \(Y\) are said to be
\textbf{independent} if 
\[P(X \in A, Y \in B):=P(X \in A)P(Y \in B),\]
for any \(A, B \subset \mathbb{R}\).

Equivalently, \(x\) and \(y\) are independent iff 
\[F_{X,Y}(x,y)=F_X(x)F_Y(y).\]

\marker{Proposition 6.3} Random variables \(X\) and \(Y\) are
\textbf{independent} if and only if there exist functions \(g,h:
\mathbb{R}\rightarrow \mathbb{R}\) such that for all \(x, y\in \mathbb{R}\), we
have 
\[f_{X, Y}(x, y) = h(x)g(x).\]

\subsection{Sums of Independent Random Variables}

\marker{Convolution} Let \(X\) and \(Y\) be \textbf{continuous and independent}. We have:
\[f_{X,Y}(x,y) = f_X(x)f_Y(y)\]
for \(x, y \in \mathbb{R}\). Then, we have:
\begin{align*}
F_{X+Y}(a) &= P(X + Y \leq a)\\
           &= \int_{-\infty}^{\infty}F_X(a-y)f_Y(y)\ \mathrm{d}y \\ 
           &= \int_{-\infty}^{\infty}F_Y(a-x)f_X(x)\ \mathrm{d}x
\end{align*}
This is called the \textbf{convolution} of \(f_X\) and \(f_Y\).

\marker{Sum of Gamma} Assume
that \(X\sim \operatorname{Gamma}\left(\alpha,\lambda\right)\) and \(Y \sim
\operatorname{Gamma}\left(\lambda\right)\), then 
\[X + Y \sim \operatorname{Gamma}\left(\alpha+\beta, \lambda\right).\]

\marker{Beta Function} \[\operatorname{B}\left(\alpha,\beta\right) =
\dfrac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha + \beta)} = \int_{0}^{1}\
(1-u)^{\alpha-1}u^{\beta - 1}\mathrm{d}u.\]

\marker{Sum of Normal} If 
\(X_i, i = 1, \cdots, n\) are independent random variables that are normally
distributed with respective parameters \(\mu_i\), \(\sigma_i^2\), \(i = 1,
\cdots, n\) then 
\[\sum\limits_{i=1}^{n}X_i \sim N\left(\sum\limits_{i=1}^{n}\mu_i,
\sum\limits_{i=1}^{n}\sigma_i^2\right).\]

\marker{Sum of Gamma} If \(X \equiv
\operatorname{Poisson}\left(\lambda\right)\), \(Y \sim
\operatorname{Poisson}\left(\mu\right)\), and \(X\) and \(Y\) are independent,
then the \textbf{p.m.f.} of \(X + Y\):
\begin{align*}
  P(X + Y = n) &= \sum\limits_{k=0}^{n}P(X = k, Y = n-k) \\ 
               &= \dfrac{\exp[-(\lambda + \mu)](\lambda + \mu)^n}{n!}
\end{align*}
The sum of two independent Poisson random variables is \textbf{still Poisson}, 
the mean is the \textbf{sum of the means}.

\marker{Sum of Binomial (same prob)} \(X\sim
\operatorname{Bin}\left(n,p\right)\), \(Y\sim
\operatorname{Bin}\left(m,p\right)\), where \(X\), \(Y\) are independent. Then,
the \textbf{p.m.f.} of \(X + Y\):
\begin{align*}
  P(X + Y = k) &= \sum\limits_{i=0}^{k}P(X=i, Y = k-i) \\ 
               &= \binom{n+m}{k}p^k(1-p)^{n+m-k}
\end{align*}
The sum of 2 indepndent binomial random variables with the \textbf{same success
probability}, \(p\), is still binomial, with parameters \(n+m\) and \(p\).

\subsection{Conditional distributions: Discrete Case}

\marker{Conditional P.M.F.} of \(X\) given \(Y = y\):
\[P(X = x|Y = y) = \dfrac{P(X = x, Y = y)}{P(Y = y)} =
\dfrac{p_{X,Y}(x,y)}{p_Y(y)},\]
for all values \(Y\) such that \(p_Y(y) > 0\).

\marker{Conditional D.F.} of \(X\) given \(Y = y\):
\[F_{X|Y}(x|y) = P(X \leq x|Y = y) = \sum_{a\leq x}p_{X|Y}(a|y).\]

\marker{Proposition 6.6} If \(X\) is independent of \(Y\), then the
\textbf{conditional p.m.f.} of \(X\) given that \(Y = y\) is the same as the
\textbf{marginal p.m.f.} of \(X\) for every \(Y\) such that \(p_Y(y)> 0\).

\subsection{Conditional distributions: Continuous Case}

\marker{Conditional P.D.F.} of \(X\) given \(Y = y\):
\[f_{X|Y}(x|y) = \dfrac{f_{X,Y}(x,y)}{f_Y(y)},\]
for all values \(y\) such that \(f_Y(y) > 0\).

\marker{Conditional D.F.} of \(X\) given \(Y = y\):
\[F_{(X|Y)}(x|y) = P(X \leq x |Y=y) = \int_{-\infty}^{x}f_{X|Y}(t|y)\
\mathrm{d}t.\]

\marker{Proposition 6.7} If \(X\) is independent of \(Y\), then the
\textbf{conditional p.d.f.} of \(X\) given that \(Y = y\) is the same as the
\textbf{marginal p.d.f.} of \(X\) for every \(Y\) such that \(f_Y(y)> 0\).

\marker{Bivariate Normal Distribution} Random variables \(X\), \(Y\) have a
\textbf{bivariate normal distribution} if, for constants \(\mu_x, \mu_y,
\sigma_x, \sigma_y> 0\), \(-1 < \rho < 1\), there \textbf{joint density
function} is given, for all \(-\infty < x, y < \infty\), by
\[f_{X, Y}(x, y) := \dfrac{1}{2\pi\sigma_x\sigma_y\sqrt{1-\rho^2}} \times
e^\text{exponent}\]
where exponent is:
\[-\dfrac{\left[\left(\dfrac{x-\mu_x}{\sigma_x}\right)^2 +
\left(\dfrac{y-\mu_y}{\sigma_y}\right)^2 -
2\rho\dfrac{(x-\mu_x)(y-\mu_y)}{\sigma_x\sigma_y}.\right]}{2(1-p^2)}.\]


\subsection{Joint Probability Distribution of Functions of Random Variables}

\marker{Proposition 6.8} Assume that the following conditions are satisfied:
\begin{enumerate}
  \item Let \(X\) and \(Y\) be \textbf{jointly continuously} distribtued random
    variables with known joint density function.
  \item Let \(U\) and \(V\) be givven functions of \(X\) and \(Y\) in the
    form \[U = g(X, Y),\quad V = h(X, Y)\]. And we can \textbf{uniquely} solvve
    for \(X\) and \(Y\) in terms of \(U\) and \(V\), i.e. say \(x = a(u, v)\)
    and \(y = b(u, v)\).
  \item The functions \(g\) and \(h\) have \textbf{continuous partial
    derivatives} at all points \((x, y)\) and \[J(x,y):=\begin{vmatrix}
      \dfrac{\partial g}{\partial x} & \dfrac{\partial g}{\partial y} \\
      \dfrac{\partial h}{\partial x} & \dfrac{\partial h}{\partial y}
    \end{vmatrix} = \dfrac{\partial g}{\partial x} \dfrac{\partial
    h}{\partial y} - \dfrac{\partial g}{\partial y} \dfrac{\partial
  h}{\partial x} \neq 0\] for all \((x, y)\).
\end{enumerate}
Then, the \textbf{joint probability density function} of \(U\) and \(V\) is
given by 
\[f_{U,V}(u,v) = f_{X,Y}(x,y)|J(x,y)|^{-1}\]
where \(x = a(u, v)\) and \(y = b(u, v)\), (check point 2).

\marker{Proposition 6.9} When the joint density function of the \(n\) random
variables \(X_1, X_2, \cdots, X_n\) is given and we want to compute the joint
density function of \(Y_1, Y_2, \cdots, Y_n\) where \(Y_i = g_i(X_1, X_2,
\cdots, X_n)\). We assume that the function \(g_j\) have continuous partial
derivatives and the Jacobian determinant \(J(x_1, x_2, \cdots, x_n)\neq 0\)
at all points \((x_1, x_2, \cdots, x_n)\), where 
\[
  J(x_1, x_2, \cdots, x_n) = \begin{vmatrix}
    \dfrac{\partial g_1}{\partial x_1} & \dfrac{\partial g_1}{\partial x_2} &
    \cdots & \dfrac{\partial g_1}{\partial x_n} \\
    \dfrac{\partial g_2}{\partial x_1} & \dfrac{\partial g_2}{\partial x_2} &
    \cdots & \dfrac{\partial g_2}{\partial x_n} \\
    \vdots & \vdots & \ddots & \vdots \\
    \dfrac{\partial g_n}{\partial x_1} & \dfrac{\partial g_n}{\partial x_2} &
    \cdots & \dfrac{\partial g_n}{\partial x_n} \\
  \end{vmatrix}
\]
Further, we suppose that the equations \(y_i\)\(=\)\(g_i(x_1, x_2, \cdots, x_n)\)
have a \textbf{unique} solution for \(x_1, x_2, \cdots, x_n\), say 
\(x_i = h_i(y_1, y_2, \cdots, y_n)\). Then, the \textbf{joint density function}
of \(Y_1, Y_2, \cdots, Y_n\) is given by. 
\(f_{Y_1, Y_2, \cdots, Y_n}\)\((y_1, y_2,
\cdots, y_n)\) \(=\) \(f_{X_1, X_2, \cdots,
X_n}(x_1, x_2, \cdots, x_n)|J(x_1, x_2, \cdots, x_n)|^{-1}\)
where \(x_i = h_i(y_1, y_2, \cdots, y_n)\)


\subsection{Jointly Distributed Random Variables: $n \geq 3$}

\[F_{X, Y, Z}(x, y, z) = P(X \leq x, Y \leq y, Z \leq z).\]

\marker{Proposition 6.10} For jointly continuous random variables, the
following three conditions are equivalent:
\begin{itemize}
  \item Random variables \(X, Y, Z\) are \textbf{independent}.
  \item For all \(x,y,z \in \mathbb{R}\), we have \[f_{X, Y, Z}(x,y,z) =
      f_X(x)f_Y(y)f_Z(z).\]
  \item For all \(x, y, z \in \mathbb{R}\), we have 
    \[F_{X, Y, Z}(x, y, z) = F_X(x)F_Y(y)F_Z(z).\]
\end{itemize}

\marker{Proposition 6.11} Random variables \(X\), \(Y\), and \(Z\) are
independent if and only if there exist functions \(g_1, g_2,
g_3:\mathbb{R}\rightarrow \mathbb{R}\) such that for all \(x, y, z \in
\mathbb{R}\), we have
\[f_{X, Y, Z}(x, y, z) = g_1(x)g_2(y)g_3(z).\]

\section{Properties of Expectation}

If \(a \leq X \leq b\), then \(a \leq E(X) \leq b\).

\subsection{Expectation of Sums of Random Variables}

\marker{Proposition 7.1}
\begin{itemize}
  \item If \(X\) and \(Y\) are \textbf{jointly discrete} with joint p.m.f
    \(p_{X,Y}(x,y))\), then \[E[g(X, Y)] = \sum_y\sum_xg(x,y)p_{X,Y}(x,y).\]
  \item If \(X\) and \(Y\) are \textbf{jointly continuous} with joint p.d.f 
    \(f_{X,Y}(x,y)\), then \[E[g(X, Y)] =
    \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}g(x,y)f_{X,Y}(x,y)
  \,\mathrm{d}x\mathrm{d}y.\]
\end{itemize}
Consequently:
\begin{itemize}
  \item If \(g(x,y)\geq 0\) whenever \(p_{X,Y}(x,y) > 0\), then \(E[g(X, Y)]
    \geq 0\).
  \item \(E[g(X, Y) + h(X,Y)] = E[g(X, Y)] + E[h(X,Y)]\).
  \item \(E[g(X) + h(Y)] = E[g(X)] + E[h(Y)]\).
  \item \textbf{Monotone property}: if jointly distributed random variables
    \(X\) and \(Y\) satisfy \(X \leq Y\), then \(E[X] \leq E[Y]\).
  \item \textbf{Special case}: mean of sum is sum of means: \(E[X + Y] = E[X]
    + E[Y]\).
\end{itemize}

\subsection{Covariance, Variance of Sums, and Correlation}

\marker{Definition 7.2} The \textbf{covariance} of random variables \(X\) and
\(Y\), denoted by \(\operatorname{Cov}\left(X,Y\right)\), is defined by 
\[\operatorname{Cov}\left(X,Y\right)= E[(X-\mu_X)(Y-\mu_Y)],\]
where \(\mu_X\) and \(\mu_Y\) denote the means of \(X\) and \(Y\) respectively.
\begin{itemize}
  \item If \(\operatorname{Cov}\left(X,Y\right)\neq 0\), then \(X\) and \(Y\)
    are \textbf{correlated};
  \item If \(\operatorname{Cov}\left(X,Y\right)= 0\), then \(X\) and \(Y\) are
    \textbf{uncorrelated}.
\end{itemize}
Alternative formula:
\[Cov(X, Y) = E(XY) - E(X)E(Y).\]

\marker{Proposition 7.2} If \(X\) and \(Y\) are \textbf{independent}, then for
any functions \(g, h: \mathbb{R}\rightarrow \mathbb{R}\), we have 
\[E[g(X)h(Y)] = E[g(X)]E[h(Y)].\]

\marker{Corollary 7.3} If \(X\) and \(Y\) are \textbf{independent}, then
\(\operatorname{Cov}\left(X,Y\right)= 0\).

\marker{Properties of Covariance}
\begin{itemize}
  \item \(\operatorname{Var}\left(X\right)=\operatorname{Cov}\left(X,X\right)\)
  \item \(\operatorname{Cov}\left(X,Y\right)=\operatorname{Cov}\left(Y,
    X\right)\)
  \item \(\operatorname{Cov}\left(\sum\limits_{i=1}^{n}a_iX_i,
    \sum\limits_{j=1}^m b_jY_j\right) =
    \sum\limits_{i=1}^{n}\sum\limits_{j=1}^m a_ib_j\operatorname{Cov}\left(
    X_i,Y_j\right).\)
\end{itemize}

\marker{Variance of a Sum}
\[\operatorname{Var}\left(\sum\limits_{k=1}^{n}X_k\right) =
\sum\limits_{k=1}^{n}\operatorname{Var}\left(X_k\right)+2\sum_{1 \leq i < j
\leq n} \operatorname{Cov}\left(X_i,X_j\right)\]
When \(X_1, X_2, \ldots, X_n\) are \textbf{independent}, we have
\[\operatorname{Var}\left(\sum\limits_{k=1}^{n}X_k\right) =
\sum\limits_{k=1}^{n}\operatorname{Var}\left(X_k\right).\]

\marker{Correlation Coefficient} The \textbf{correlation coefficient} of
random variables \(X\) and \(Y\), denoted by \(\rho_{X,Y}\), is defined by
\[\rho_{X,Y} = \frac{\operatorname{Cov}\left(X,Y\right)}
{\sqrt{\operatorname{Var}\left(X\right)\operatorname{Var}\left(Y\right)}}.\]

\marker{Proposition 7.3} \(-1 \leq \rho_{X,Y} \leq 1\).

\subsection{Conditional Expectation}

\marker{Definition 7.3} 
\begin{itemize}
  \item If \(X\) and \(Y\) are \textbf{jointly distributed
discrete} random variables, then 
\[E\left[X\mid Y=y\right]=\sum\limits_{x}x\cdot p_{X\mid Y}\left(x\mid
y\right).\]
  \item If \(X\) and \(Y\) are \textbf{jointly distributed continuous} random
    variables, then
\[E\left[X\mid Y=y\right]=\int_{-\infty}^{\infty}x\cdot f_{X\mid Y}\left(x\mid
y\right)\,\mathrm{d}x.\]
\end{itemize}

\marker{Cond E of a Function}
\begin{itemize}
  \item \textbf{Discrete}: \(E[g(X)|Y=y] = \sum_x g(x)p_{X|Y}(x|y)\);
  \item \textbf{Continuous}: \(E[g(X)|Y=y] = \int_{-\infty}^{\infty}
    g(x)f_{X|Y}(x|y)\,\mathrm{d}x\).
\end{itemize}
Therefore, \(E\left[\sum\limits_{k=1}^{n}X_k | Y=y\right] =
\sum\limits_{k=1}^{n}E[X_k|Y = y]\).

\marker{Proposition 7.5 Law of Total Variance} 
\[\operatorname{Var}\left(X\right)=E\left[\operatorname{Var}\left(X\mid
Y\right)\right]+\operatorname{Var}\left(E\left[X\mid Y\right]\right).\]

\subsection{Moment Generating Functions}

\marker{Definition 7.4} The \textbf{moment generating function} (MGF) of a
random variable \(X\), denoted by \(M_X\), is defined by
\begin{itemize}
  \item \textbf{Discrete}: \(M_X(t) = E[e^{tX}] = \sum_x e^{tx}p_X(x)\);
  \item \textbf{Continuous}: \(M_X(t) = E[e^{tX}] = \int_{-\infty}^{\infty}
    e^{tx}f_X(x)\,\mathrm{d}x\).
\end{itemize}

\marker{Proposition 7.6 Multiplicative Property} If \(X\) and \(Y\) are
independent random variables, then
\[M_{X+Y}(t) = M_X(t)M_Y(t).\]

\marker{Proposition 7.7 Uniqueness Property} If \(X\) and \(Y\) are random
variables with their moment generating functions \(M_X(t)\) and \(M_Y(t)\)
respectively. Suppose that there exists an \(h < 0\) such that 
\[M_X(t) = M_Y(t), \quad \forall t \in (-h, h).\]
Then \(X\) and \(Y\) have the same distribution, i.e. \(F_X = F_Y\) and 
\(f_X = f_Y\).


\subsection{Joint Moment Generating Functions}

For any \(n\) random variables \(X_1, X_2, \ldots, X_n\), the \textbf{joint
moment generating function}, \(M_{X_1, \cdots, X_n}(t_1, \cdots, t_n)\), is
defined for all real values of \(t_1, \cdots, t_n\) by
\[M_{X_1, \cdots, X_n}(t_1, \cdots, t_n) = E\left[\exp({t_1X_1 + \cdots +
t_nX_n})\right].\]
The \textbf{individual} moment generating function can be obtained by
setting the other \(t_j\)'s to zero:
\[M_{X_i}(t_i) = M_{X_1, \cdots, X_n}(0, \cdots, t_i, \cdots, 0).\]

\marker{Proposition 7.8 Independence of Mean and Variance from Normal Sample} 
Let \(X_1, X_2, \ldots, X_n\) be independent and identically distributed normal
random variables with mean \(\mu\) and variance \(\sigma^2\). Then the sample
mean \(\overline{X}\) and the sample variance \(S^2\) are independent, and
\(\overline{X}\sim N\left(\mu, \sigma^2/n\right)\) and \((n-1)S^2/\sigma^2
\sim \chi^2(n-1)\).

\section{Limit Theorems}

\subsection{Introduction}

\subsection{Chebyshev's Inequality and the Weak Law of Large Numbers}

\marker{Proposition 8.1 Markov's Inequality} Let \(X\) be a
\textbf{nonnegative} random variable. For \(a > 0\), we have 
\[P(X \geq a)\leq \dfrac{E[X]}{a}.\]

\marker{Proposition 8.2 Chebyshev's Inequality} Let \(X\) be a random variable
with mean \(\mu\) and variance \(\sigma^2\). For \(a > 0\), we have
\[P(|X-\mu| \geq a) \leq \dfrac{\sigma^2}{a^2}.\]

\marker{Proposition 8.3} If \(\operatorname{Var}\left(X\right) = 0\), then the
random variable \(X\) is a constant.

\marker{Theorem 8.1 The Weak Law of Large Numbers} Let \(X_1, X_2, \cdots\) be
a sequence of independent and identically distributed random variables with
common mean \(\mu\). Then, for any \(\epsilon > 0\), we have
\[P\left(\left|\dfrac{X_1+X_2+\cdots+X_n}{n}-\mu\right|\geq
\epsilon\right)\rightarrow 0\]
as \(n\rightarrow\infty\).

\subsection{The Central Limit Theorem}

Let \(X_1, X_2, \cdots\) be a sequence of independent and identically
distributed random variables, each having mean \(\mu\) and variance
\(\sigma^2\). Then the distribution of 
\[\dfrac{X_1+X_2+\cdots+X_n-n\mu}{\sigma\sqrt{n}}\]
tends to the standard norm as \(n \rightarrow \infty\). That is 

\begin{align*}
&\lim\limits_{n\rightarrow
\infty}P\left(\dfrac{X_1+X_2+\cdots+X_n-n\mu}{\sigma\sqrt{n}}\right) \\ 
  =&\dfrac{1}{\sqrt{2\pi}}\int_{x}^{-\infty}\exp(-t^2/2)\ \mathrm{d}t
\end{align*}

\subsection{The Strong Law of Large Numbers}

\marker{Theorem 8.2 The Strong Law of Large Numbers} Let \(X_1, X_2, \cdots\)
be a sequence of independent and identically distributed random variables with
common mean \(\mu\). Then, with probability 1, we have
\[\lim\limits_{n\rightarrow\infty}\dfrac{X_1+X_2+\cdots+X_n}{n}=\mu.\]


\subsection{Other Inequalities}

\marker{Proposition 8.4 One-sided Checbychev's Inequality} Let \(X\) be a
random variable with mean \(0\) and finite variance \(\sigma^2\). Then, for
\(a > 0\), we have
\[P(X \geq a) \leq \dfrac{\sigma^2}{\sigma^2+a^2}.\]

\marker{Proposition 8.5 Jensen's Inequality} If \(g(x)\) is a \textbf{convex}
function, then 
\[g(E[X]) \leq E[g(X)]\]
provided that expectations exist and are finite.

A function is convex if either of the following equivalent conditions hold:
\begin{enumerate}
  \item for all \(0 \leq p \leq 1\) and for all \(x_1, x_2 \in R_X\), 
    \[g(px_1 + (1-p)x_2)\leq pg(x_1) + (1-p)g(x_2).\]
  \item differentiable function: convex of interval if and only if 
    \[g(x) \geq g(y) + g'(y)(x-y)\]
    for all \(x, y\) in the interval.
  \item A twice differentiable function is convex if and only if its second
    derivative is nonnegative.
\end{enumerate}























\end{multicols*}
\end{document}
